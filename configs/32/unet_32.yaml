model:
  learning_rate: 1.0e-4
  weight_decay: 1.0e-4
  optimizer: 'AdamW' # ['Adam', 'AdamW']
  lr_scheduler: 'Constant' # ['Constant', 'Cosine']
  #lr_warmup: 1000 # needed if use cosine lr_scheduler
  #lr_cosine_period: 24000 # match dataset_size/batch_size*max_epochs
  model: 'UNet' # ['default', 'UNet', 'VAE']
  num_layers: 4 # layer number of UNet
  base_filters: 64 # base filter after first convolutional layer
  blocks_per_layer: 2 # number of convolutional blocks in each layer
  init_dim: 3 # 3 for without density field in tensor
  reversed: False # if True, predict ZA from FastPM
  compressed: False # if True, compress input data
  compression_type: 'arcsinh' # ['arcsinh', 'tanh', 'sqrt']
  compression_factor: 24 # compression factor
  eul_loss: False # if True, use Eulerian loss
  eul_loss_scale: 1.0 # scale factor for Eulerian loss
  lag_loss_scale: 1.0 # scale factor for Lagrangian loss

data:
  dataset_type: 'hdf5' # ['raw', 'huggingface', hdf5]
  train_csv: '/home/user/ckwan1/ml_project/sobol_32_hdf5/train.csv'
  val_csv: '/home/user/ckwan1/ml_project/sobol_32_hdf5/val.csv'
  batch_size: 32
  num_workers: 8
  augment: True # if True, use data augmentation
  density: False # if True, use density field in tensor

trainer:
  max_epochs: 1000
  gpus: 4
  num_nodes: 1
  ckpt_path: '/home/user/ckwan1/ml_project/field2field/new_checkpoints/unet_32/last.ckpt'
wandb:
  project: "field2field"
  entity: "brianwan221-the-chinese-university-of-hong-kong"
  name: "default_unet"
  id: "b3oaehor"
  resume: 'must' # 'must' to resume from the last checkpoint
  group: "32"

